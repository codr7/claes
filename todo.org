3268
1208

mistral 8x7b
https://medium.com/vendi-ai/efficiently-run-your-fine-tuned-llm-locally-using-llama-cpp-66e2a7c51300

* add Map type
** add {} reader
** Value = map<Cell, Cell>
** copy benchmark from jalang
** implement js
*** write object

* update llama.claes to use js with map literal
** quote entire map and remove quotes from names

* add ask method
** optional prompt arg
** read line (repl)
*** return optional string
** use to implement repl loop in llama.claes

* add json parser
** extract answer in llama.claes

* --

* add support for f64 > 1 in reader

* add safe mode
** command line arg
** disallow anything that changes the machine
** remove import/say/dump/stop/etc

* add composition using pipe character
** check jalang

* add import macro
** register core/curl in root env
*** add VM.root_env
** remove default curl import
** use in llama.claes

* define <= and >= in core eval block
** use vararg/splat
** add tests

* add vm.arity vector
** write failing test with nested splats
** add PUSH_ARITY op
*** push initial arity
** pop in call ops
** skip zeroing in call ops

* ifdef curl lib
** CMake
** libs/Curl

* add len method
** add Cell.len()
** return 1 default
** override for i64, string, vector, pair (2)

* add Type.peek/pop
** self default
*** set nil on pop
** first för pair
** peek/pop för vector/string
** pop for i64
** add stacks to readme

* implement Pair::iter

* replace Rec with move/erase

* add composition support
** check jalang/dot
** use |

* add filter macro
* add reduce macro
* add map macro

* aoc1

* use vm alloc for Ref imps
* use vm alloc for Env imps

* add incr macro
** add increment op

* rebind updated parent Expr envs in Env constructor
** replaces default create of new env in Expr
